{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efz83OboW1lU"
      },
      "source": [
        "# Show-Us-Your-RAG-Skillz\n",
        "## Alessandro Corvi\n",
        "\n",
        " In order to develop a fast and reliable RAG which impersonates a fast-foot cashier some design choices had to be made. The most impactful one is for sure to use the LLM framework `haystack` in order to facilitate the whole process and obstain a more scalable product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SWmrYElAayTl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install farm-haystack[inference]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5BUgeDfcLPI"
      },
      "source": [
        "The choice of the model also has a great impact over the system performance and accuracy. Mixtral-8x7B-Instruct-v0.1 will be used thanks to its fenomenal scores when compared even to a bigger model such as LLama 13B.\n",
        "\n",
        "In order to obtain the model an huggingface token is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pIWRabo7dEaI"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import PromptNode\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naBfUr87jDfU",
        "outputId": "5dce73fc-5e8f-45f9-e5db-7b729e59ce60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please input your HF access token: ··········\n"
          ]
        }
      ],
      "source": [
        "HF_TOKEN = getpass(\"Please input your HF access token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtvDMWJ6dKsM",
        "outputId": "7adea75a-cc86-4adc-aa4a-42b40e76b1eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "pn = PromptNode(model_name_or_path=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "                max_length=800,\n",
        "                api_key=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc9_DvoNlCQv"
      },
      "source": [
        "## Load and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv2BeRdWa8zY"
      },
      "source": [
        "After loading `menu.json` some work has to be done. The json structures when parsed creates a nested dict which if left untouched would result in extremely poor accuracy from the LLM.\n",
        "In order to avoid this issue we expand the json hierarchy by splitting it in more focused instances to then create numerical vector representations. By using the embedding model `bert` the information inside the json are transformed into vectors in order for the LLM to maximise semantic understanding by using such a structure.\n",
        "\n",
        "#### Note:\n",
        "Even if the vector database created is not directly maintanable here it can be interacted with via the StreamLit application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX0J8QhkRlYP",
        "outputId": "dcfeb507-7f8d-4e2d-d72f-2bd59c5ce811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Updating Embedding:   0%|          | 0/91 [00:00<?, ? docs/s]\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:02<00:04,  2.35s/ Batches]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:03<00:01,  1.67s/ Batches]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:04<00:00,  1.48s/ Batches]\n",
            "Documents Processed: 10000 docs [00:04, 2201.40 docs/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from haystack import Document\n",
        "from haystack.document_stores import InMemoryDocumentStore\n",
        "from haystack.nodes import EmbeddingRetriever, PromptTemplate\n",
        "\n",
        "with open('menu.json', 'r') as file:\n",
        "    menu_data = json.load(file)\n",
        "\n",
        "def create_description(item_name, details):\n",
        "    description = f\"{item_name}: \"\n",
        "    if isinstance(details, list):\n",
        "        description += f\"{details[0]}, Price: {details[1]}, \"\n",
        "        if len(details) > 2 and isinstance(details[2], dict):\n",
        "            for key, value in details[2].get(\"nutritionalInfo\", {}).items():\n",
        "                description += f\"{key}: {value}, \"\n",
        "            description += f\"Available: {details[2]['available']}\"\n",
        "    elif isinstance(details, dict):\n",
        "        description += f\"Name: {details.get('name', '')}, Price: {details.get('price', '')}\"\n",
        "        if 'contents' in details:\n",
        "            description += \", Contents: [\"\n",
        "            for item in details['contents']:\n",
        "                if isinstance(item, list):\n",
        "                    description += f\"({item[0]}, {item[1]}), \"\n",
        "                elif isinstance(item, dict):\n",
        "                    description += f\"({item.get('from', '')}, Size: {item.get('size', '')}), \"\n",
        "            description = description.rstrip(\", \")\n",
        "            description += \"]\"\n",
        "    return description\n",
        "\n",
        "documents = []\n",
        "for category, items in menu_data.items():\n",
        "    for item_name, details in items.items():\n",
        "        description = create_description(item_name, details)\n",
        "        documents.append(Document(content=description))\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "document_store.write_documents(documents)\n",
        "\n",
        "retriever = EmbeddingRetriever(document_store=document_store, embedding_model=\"deepset/sentence_bert\",progress_bar=False)\n",
        "\n",
        "document_store.update_embeddings(retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM and a pipeline for it are then created and customised."
      ],
      "metadata": {
        "id": "Co8VkSCP5Wpk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HKStelv4pr_f"
      },
      "outputs": [],
      "source": [
        "qa_template = PromptTemplate(prompt=\n",
        "  \"\"\"<s>[INST] You are working as a drive-in cashier at a fast-food restaurant act like such, be accomodating and be careful about what you are asked. Using only the information contained in the context, answer the question. If the question is not fast-food related remember the customer your function.\n",
        "  If the answer cannot be deduced from the context, answer \\\"I don't know.\\\"\n",
        "  Context: {join(documents)};\n",
        "  Question: {query}\n",
        "  [/INST]\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V7M8_QmfqGO7"
      },
      "outputs": [],
      "source": [
        "prompt_node = PromptNode(model_name_or_path=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "                         api_key=HF_TOKEN,\n",
        "                         default_prompt_template=qa_template,\n",
        "                         max_length=5500,\n",
        "                         model_kwargs={\"model_max_length\":8000})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IzD5jw84oTVr"
      },
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
        "rag_pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RHzz1UQlt1M5"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "print_answer = lambda out: pprint(out[\"results\"][0].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuzhiKJdv3bx"
      },
      "source": [
        "### Here queries can be run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqYdpQyQw0O_",
        "outputId": "119158c4-d146-48c1-ee45-8fd8ed594a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 19.32 Batches/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"I'm sorry, I need a bit more information to answer your question. The menu \"\n",
            " 'items listed have calorie information, but \"Colonel\" could refer to multiple '\n",
            " 'items such as the Colonel Stacker, Colonel Burger, or even a meal that '\n",
            " 'includes one of those items. Could you please clarify which \"Colonel\" item '\n",
            " \"you're interested in? That way, I can provide a more accurate answer. Thank \"\n",
            " 'you!')\n",
            "Execution time: 351.99642181396484ms\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "answer=rag_pipeline.run(query=\"How many calories does the Colonel have?\")\n",
        "end_time = time.time()\n",
        "\n",
        "print_answer(answer)\n",
        "total_time = end_time - start_time\n",
        "print(\"Execution time: \" + str(total_time*1000) + \"ms\")\n",
        "# Note execution time fluctuates when the application has just been launched."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}